\documentclass[a4paper, 10pt]{article}
\linespread{1.33}
\input{../preamble.tex}
\input{../usercommand.tex}

\newcommand\lecturenumber{13}
\newcommand\lecturedate{Jan 17, 2025}

\pagestyle{fancyplain}
\headheight 40pt
\lhead{Lecture \lecturenumber\\CNBC Deep Learning Subgroup}
\rhead{Riemannian Geometry for Deep Learning \\\lecturedate}
\cfoot{Fall 2024, SNU}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}
\setcounter{section}{7}
\section{Lie Groups and Lie Algebras}

\setcounter{subsection}{2}
\subsection{Examples}

\setcounter{theorem}{13}
\begin{example}
    Let $G = (\mbb{R}, +)$. Then, identity is 0 and $L_{a}x = a + x$. What is the tangent vector at $x = 0$? Let's compute the following equation's both sides
    \[ L_{a\ast} \left.\frac{d}{dx}\right|_{x=0} = k \cdot \left.\frac{d}{dx}\right|_{x=a} \]
    and find value of the constant $k$. To evaluate $k$, apply both sides to $x$. Then,
    \[ k \cdot \left.\frac{d}{dx}\right|_{x=a}x = k \]
    and
    \[ L_{a\ast} \left( \left.\frac{d}{dx}\right|_{x=0} \right)x = \left.\frac{d}{dx}\right|_{x=0} (x \circ L_{a}) = \left.\frac{d}{dx}\right|_{x=0} (a+x) = 1 \]
    So, $k=1$ and
    \[ \boxed{ L_{a\ast} \left.\frac{d}{dx}\right|_{x=0} = \left.\frac{d}{dx}\right|_{x=a} } \]
    Therefore, $\displaystyle{\frac{d}{dx}}$ is a left-invariant vector field on $\mbb{R}$.
    
    Moreover, left-invariant vector fields on $\mbb{R}$ are constant multiples of $\displaystyle{\frac{d}{dx}}$.
\end{example}

\begin{remark}
    $\displaystyle{X = \frac{\partial}{\partial\theta}}$ is the unique left-invariant vector field on $G = \mrm{SO}(2) = \{e^{i\theta} \,|\, 0 \leq \theta \leq 2\pi\}$. $\mbb{R}$ and $\mrm{SO(2)}$ share the common Lie algebra.
\end{remark}

\newpage

% ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====

\begin{example}[Left-invariant fields on $\mrm{GL}(n,\mbb{R})$]
    An element of $\mrm{GL}(n,\mbb{R})$ has $n^{2}$ entries, $x^{ij}$, of the matrix.
    \begin{itemize}
        \item[-] Unit element: $e = \mbf{I}_{n} = \delta^{ij}$
        \item[-] Let $g = \{x^{ij}(g)\}$ and $a = \{x^{ij}(a)\} \in \mrm{GL}(n,\mbb{R})$. The left-translation is
        \[ L_{a}g = ag = \sum_{k} x^{ik}(a)x^{kj}(g) \]
        \item[-] Take a vector $\displaystyle{V = V^{ij} \left.\frac{\partial}{\partial x^{ij}}\right|_{e} \in T_{e}G}$. The left-invariant vector field generated by $V$ is
        \begin{align*}
            X_{V}|_{g} &= L_{g\ast}V = V^{ij}\underbrace{\left(\left.\frac{\partial}{\partial x^{ij}}\right|_{e}x^{kl}(g)x^{lm}(e)\right)}_{\text{pushforward formula}} \underbrace{\left.\frac{\partial}{\partial x^{km}}\right|_{g}}_{\text{basis at }g} \\
            &= V^{ij}x^{kl}(g)\delta^{l}{}_{i}\delta^{m}{}_{j}\left.\frac{\partial}{\partial x^{km}}\right|_{g} = x^{ki}(g)V^{ij}\left.\frac{\partial}{\partial x^{kj}}\right|_{g} \\
            &= \boxed{(gV)^{kj}\left.\frac{\partial}{\partial x^{kj}}\right|_{g}}
        \end{align*}
        As we expected, left-translation of \tit{vectors} in $T_{e}\mrm{GL}(n,\mbb{R})$ is represented by usual matrix multiplication.
        \item[-] Now we compute Lie brackets. Take two vectors
        \[ V = V^{ij} \left.\frac{\partial}{\partial x^{ij}}\right|_{e} \quad\text{and}\quad W = W^{ij} \left.\frac{\partial}{\partial x^{ij}}\right|_{e} \]
        Then, the Lie bracket of two vector fields $X_{V}$ and $X_{W}$ generated by $V$ and $W$, respectively, becomes
        \begin{align*}
            [X_{V}, X_{W}]|_{g} &= x^{ki}(g)V^{ij}\left.\frac{\partial}{\partial x^{kj}}\right|_{g} x^{ca}W^{ab}\left.\frac{\partial}{\partial x^{cb}}\right|_{g} - (V \leftrightarrow W) \\
            &= x^{ki}(g)[V^{ij}W^{jb}-W^{ij}V^{jb}]\left.\frac{\partial}{\partial x^{kb}}\right|_{g} \\
            &= x^{ij}(g)[V^{jk}W^{kl}-W^{jk}V^{kl}]\left.\frac{\partial}{\partial x^{il}}\right|_{g} \quad(\because \;\text{dummy index change}) \\
            &= (g[V,W])^{ij}\left.\frac{\partial}{\partial x^{ij}}\right|_{g}
        \end{align*}
    \end{itemize}
    In summary, the following holds for any matrix groups.
    \[ \boxed{L_{g\ast}V = gV} \quad\text{and}\quad \boxed{[X_{V},X_{W}]|_{g} = L_{g\ast}[V,W] = g[V,W]} \]
\end{example}

\newpage

% ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====

\begin{example}[Lie groups and algebras of matrix groups]
    \hphantom{.}
    \begin{itemize}
        \item[(a)] $\mathfrak{gl}(n,\mbb{R})$: Lie algebra of $\mrm{GL}(n,\mbb{R})$.
        \begin{itemize}
            \item[-] Consider the parametrized curve $c \,:\, (-\epsilon,\epsilon) \rightarrow \mrm{GL}(n,\mbb{R})$ with $c(0) = \mbf{I}_{n}$.
            \item[-] If $\epsilon$ is small enough, this curve can be approximated by $c(s) = \mbf{I}_{n} + sA + \mca{O}(s^{2})$ near $s = 0$, where $A$ is an $n \times n$ matrix of real entries (without further constraints).
            \item[-] For small $s$, $\det c(s)$ cannot vanish, so $c(s) \in \mrm{GL}(n,\mbb{R})$.
            \item[-] The tangent vector to $c(s)$ at $\mbf{I}_{n}$ is $c'(s)|_{s=0}=A$. Therefore, $\mathfrak{gl}(n,\mbb{R})$ is the set of $n \times n$ matrices with dimension $\dim \mathfrak{gl}(n,\mbb{R}) = n^{2} = \dim \mrm{GL}(n,\mbb{R})$
        \end{itemize}
        \item[(b)] $\mathfrak{sl}(n,\mbb{R})$: Lie algebra of $\mrm{SL}(n,\mbb{R})$. Let's use the same setup as in (a). Now we have additional constraint: for the curve $c(s)$ to be in $\mrm{SL}(n,\mbb{R})$, $\det c(s) = +1$ should be satisfied.
        \[ \det c(s) = 1 + s \mrm{tr}A = 1 \;\Longrightarrow\; \boxed{\mrm{tr}A = 0} \]
        Hence, $\mathfrak{sl}(n,\mbb{R})$ is the set of $n \times n$ \tit{traceless} matrices with $\dim \mathfrak{sl}(n,\mbb{R}) = n^{2} - 1$.
        \item[(c)] $\mathfrak{o}(n)$: Lie algebra of $\mrm{O}(n)$. Now, condition for $c(s)$ becomes $c(s)^{\msf{T}}c(s) = \mbf{I}_{n}$. Differentiating both sides with respect to $s$ yields
        \[ c'(s)^{\msf{T}}c(s) + c(s)^{\msf{T}}c'(s) = 0 \]
        At $s = 0$, this reduces into $A^{\msf{T}} + A = 0$, or equivalently, $A^{\msf{T}} = -A$. Therefore, $\mathfrak{o}(n)$ is the set of skew-symmetric matrices with $\dim \mathfrak{o}(n) = \binom{n}{2}$.
        \item[(d)] $\mathfrak{so}(n) = \mathfrak{o}(n)$ since all skew-symmetric matrices are traceless\footnote{Note that we are interested only in the vicinity of the unit element, so $\mrm{O}(n)$ and $\mrm{SO}(n)$ shows no difference here.}.
        \item[(e)] We can think of same analogy for complex matrices, except they have $2n^{2}$ entries($n^{2}$ for real part and $n^{2}$ for imaginary part).
        \begin{itemize}
            \item[-] $\mathfrak{gl}(n,\mbb{C})$: the set of $n \times n$ matrices with complex entries ($\dim \mathfrak{gl}(n,\mbb{C}) = 2n^{2}$).
            \item[-] $\mathfrak{sl}(n,\mbb{C})$: the set of $n \times n$ traceless matrices with complex entries ($\dim \mathfrak{sl}(n,\mbb{C}) = 2(n^{2}-1)$).
            \item[-] $\mathfrak{u}(n)$: the set of $n \times n$ skew-Hermitian\footnote{$A^{\dagger} = -A$} matrices with complex entries ($\dim \mathfrak{u}(n) = n + 2\binom{n}{2} = n^{2}$, where $n$ comes from the imaginary part of diagonal elements).
            \item[-] $\mathfrak{su}(n)$: the set of $n \times n$ traceless skew-Hermitian matrices with complex entries ($\dim \mathfrak{su}(n) = n^{2} - 1$).
        \end{itemize}
    \end{itemize}
\end{example}



\end{document}
